"""
Cooperative Trading Detection System (ê³µëª¨ê±°ë˜ íƒì§€ ì‹œìŠ¤í…œ)
Version: 2.1
Author: Singapore Fintech Hackathon Team

ì£¼ìš” ê°œì„  ì‚¬í•­ (v2.1):
1. PnL ê³„ì‚° ì‹œ position_id ì¤‘ë³µ ì œê±° ë¡œì§ ì¶”ê°€
   - ë™ì¼ position_idê°€ ì—¬ëŸ¬ ê±°ë˜ ìŒì— ë‚˜íƒ€ë‚  ìˆ˜ ìˆì–´ ì¤‘ë³µ ì§‘ê³„ ë°©ì§€
   - AD_2.py ë¡œì§ ì°¸ê³ í•˜ì—¬ ê°œì„ 
2. SQL ì¿¼ë¦¬ ì‹œê°„ ê³„ì‚° ê°œì„  (epoch í•¨ìˆ˜ ì‚¬ìš©)

íƒì§€ ëŒ€ìƒ: ë³µìˆ˜ ê³„ì • ê°„ í˜‘ë ¥í•˜ì—¬ ë¶€ë‹¹ ì´ë“ì„ ì·¨í•˜ëŠ” íŒ¨í„´
- ë™ì‹œ ë§¤ë§¤ íŒ¨í„´
- IP ê³µìœ 
- ë„¤íŠ¸ì›Œí¬ ë¶„ì„
"""

import pandas as pd
import duckdb as dd
import json
from dataclasses import dataclass, field, asdict
from typing import Dict, List, Optional, Set, Tuple
from datetime import datetime
from pathlib import Path
from enum import Enum
import logging
from collections import defaultdict, Counter
# Detectors read model tables directly from the persistent DuckDB file

# ============================================================================
# 1. CONFIGURATION & TYPES
# ============================================================================

class RiskLevel(Enum):
    """ìœ„í—˜ë„ ë¶„ë¥˜"""
    CRITICAL = "CRITICAL"      # í™•ì‹¤í•œ ê³µëª¨
    HIGH = "HIGH"              # ë†’ì€ ì˜ì‹¬
    MEDIUM = "MEDIUM"          # ì¤‘ê°„ ì˜ì‹¬
    LOW = "LOW"                # ë‚®ì€ ì˜ì‹¬


class SanctionType(Enum):
    """ì œì¬ ìœ í˜•"""
    IMMEDIATE_CRITICAL = "IMMEDIATE_CRITICAL"      # Critical ì¦‰ì‹œ ì œì¬
    IP_SHARED_NETWORK = "IP_SHARED_NETWORK"        # IP ê³µìœ  ë„¤íŠ¸ì›Œí¬
    REPEATED_PATTERN = "REPEATED_PATTERN"          # ë°˜ë³µ íŒ¨í„´


@dataclass
class DetectionConfig:
    """íƒì§€ ì„¤ì • ë° í•˜ì´í¼íŒŒë¼ë¯¸í„°"""
    
    # ===== Filter Parameters (í•„ìˆ˜ ì¡°ê±´) =====
    max_open_time_diff_min: float = 2.0           # ìµœëŒ€ ì˜¤í”ˆ ì‹œê°„ì°¨ (ë¶„)
    max_close_time_diff_min: float = 2.0          # ìµœëŒ€ í´ë¡œì¦ˆ ì‹œê°„ì°¨ (ë¶„)
    exclude_major_symbols: bool = True             # ì£¼ìš” ì‹¬ë³¼ ì œì™¸
    major_symbols: List[str] = field(default_factory=lambda: [
        'BTCUSDT.PERP', 'ETHUSDT.PERP', 'SOLUSDT.PERP',
        'XRPUSDT.PERP', 'BNBUSDT.PERP', 'DOGEUSDT.PERP'
    ])
    
    # ===== Scoring Weights (ì ìˆ˜ ë°°ì ) =====
    weight_pnl_asymmetry: int = 35                 # PnL ë¹„ëŒ€ì¹­ì„± (í•œìª½ í° ì´ìµ)
    weight_time_proximity: int = 25                # ì‹œê°„ ê·¼ì ‘ë„
    weight_ip_sharing: int = 25                    # IP ê³µìœ 
    weight_position_overlap: int = 15              # í¬ì§€ì…˜ ê²¹ì¹¨
    
    # ===== Risk Thresholds (ìœ„í—˜ë„ íŒì • ê¸°ì¤€) =====
    critical_threshold: int = 85                   # Critical ìµœì†Œ ì ìˆ˜
    high_threshold: int = 70                       # High ìµœì†Œ ì ìˆ˜
    medium_threshold: int = 50                     # Medium ìµœì†Œ ì ìˆ˜
    
    # ===== Network Analysis Parameters =====
    min_group_size: int = 2                        # ìµœì†Œ ê·¸ë£¹ í¬ê¸°
    min_shared_ips: int = 1                        # ì œì¬ ìµœì†Œ ê³µìœ  IP ìˆ˜
    
    # ===== Output Settings =====
    output_dir: str = "output/cooperative"
    enable_detailed_logging: bool = True
    
    def to_dict(self) -> Dict:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return asdict(self)
    
    def save(self, filepath: str):
        """JSON íŒŒì¼ë¡œ ì €ì¥"""
        Path(filepath).parent.mkdir(parents=True, exist_ok=True)
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(self.to_dict(), f, indent=2, ensure_ascii=False)
        print(f"ì„¤ì • ì €ì¥ ì™„ë£Œ: {filepath}")


@dataclass
class ScoreBreakdown:
    """ì ìˆ˜ ìƒì„¸ ì •ë³´"""
    pnl_asymmetry: float = 0.0
    time_proximity: float = 0.0
    ip_sharing: float = 0.0
    position_overlap: float = 0.0
    total: float = 0.0
    
    def to_dict(self) -> Dict:
        return asdict(self)


@dataclass
class TradePair:
    """ê±°ë˜ ìŒ ì •ë³´"""
    pair_id: str
    account_id1: str
    account_id2: str
    
    # ê±°ë˜ ì •ë³´
    symbol: str
    side1: str
    side2: str
    
    # ì‹œê°„ ì •ë³´
    open_ts1: datetime
    open_ts2: datetime
    closing_ts1: datetime
    closing_ts2: datetime
    open_time_diff_sec: float
    close_time_diff_sec: float
    
    # ê±°ë˜ ìƒì„¸
    amount1: float
    amount2: float
    leverage: int
    position_id1: str
    position_id2: str
    
    # ì†ìµ ì •ë³´
    rpnl1: float
    rpnl2: float
    total_pnl: float
    pnl_winner: str  # account with positive pnl
    pnl_loser: str   # account with negative pnl
    
    # ì ìˆ˜ ë° íŒì •
    score: ScoreBreakdown = field(default_factory=ScoreBreakdown)
    risk_level: RiskLevel = RiskLevel.LOW
    
    # í•„í„° í†µê³¼ ì—¬ë¶€
    passed_filter: bool = False
    filter_failures: List[str] = field(default_factory=list)
    
    def to_dict(self) -> Dict:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        data = asdict(self)
        data['risk_level'] = self.risk_level.value
        data['open_ts1'] = self.open_ts1.isoformat()
        data['open_ts2'] = self.open_ts2.isoformat()
        data['closing_ts1'] = self.closing_ts1.isoformat()
        data['closing_ts2'] = self.closing_ts2.isoformat()
        return data


@dataclass
class CooperativeGroup:
    """ê³µëª¨ ê·¸ë£¹"""
    group_id: str
    members: List[str]
    
    # ê±°ë˜ ì •ë³´
    trade_pair_ids: List[str]
    trade_count: int
    
    # ì†ìµ ì •ë³´
    pnl_positive_sum: float
    pnl_negative_sum: float
    pnl_total: float
    
    # IP ì •ë³´
    shared_ip_count: int
    shared_ips: Dict[str, List[str]] = field(default_factory=dict)  # IP -> accounts
    
    # ì ìˆ˜ ë° íŒì •
    avg_score: float = 0.0
    max_score: float = 0.0
    risk_level: RiskLevel = RiskLevel.LOW
    
    def to_dict(self) -> Dict:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        data = asdict(self)
        data['risk_level'] = self.risk_level.value
        return data


@dataclass
class SanctionCase:
    """ì œì¬ ì¼€ì´ìŠ¤"""
    case_id: str
    sanction_type: SanctionType
    group_id: str
    account_ids: List[str]
    detection_timestamp: datetime
    
    # ì¦ê±° ë°ì´í„°
    trade_pair_ids: List[str]
    total_score: float
    risk_level: RiskLevel
    
    # IP ì •ë³´
    shared_ip_count: int = 0
    shared_ips: Dict[str, List[str]] = field(default_factory=dict)
    
    # íŒ¨í„´ ì •ë³´
    pattern_count: Optional[int] = None
    
    # ì¶”ê°€ ë©”íƒ€ë°ì´í„°
    total_pnl: float = 0.0
    pnl_positive_sum: float = 0.0
    pnl_negative_sum: float = 0.0
    evidence_summary: str = ""
    
    def to_dict(self) -> Dict:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        data = asdict(self)
        data['sanction_type'] = self.sanction_type.value
        data['risk_level'] = self.risk_level.value
        data['detection_timestamp'] = self.detection_timestamp.isoformat()
        return data


# ============================================================================
# 2. LOGGING SETUP
# ============================================================================

class DetectionLogger:
    """íƒì§€ ì‹œìŠ¤í…œ ì „ìš© ë¡œê±°"""
    
    def __init__(self, config: DetectionConfig):
        self.config = config
        self.output_dir = Path(config.output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ë¡œê·¸ íŒŒì¼ ê²½ë¡œ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = self.output_dir / f"detection_{timestamp}.log"
        
        # ë¡œê±° ì„¤ì •
        logging.basicConfig(
            level=logging.DEBUG if config.enable_detailed_logging else logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        self.logger = logging.getLogger(__name__)
        self.logger.info("="*70)
        self.logger.info("ê³µëª¨ê±°ë˜ íƒì§€ ì‹œìŠ¤í…œ ì‹œì‘")
        self.logger.info("="*70)
    
    def log_phase(self, phase_name: str):
        """ë‹¨ê³„ ì‹œì‘ ë¡œê·¸"""
        self.logger.info("")
        self.logger.info("="*70)
        self.logger.info(f"PHASE: {phase_name}")
        self.logger.info("="*70)
    
    def log_filter_result(self, total: int, passed: int, failed: int):
        """í•„í„° ê²°ê³¼ ë¡œê·¸"""
        self.logger.info(f"í•„í„° ê²°ê³¼: ì´ {total}ê±´ â†’ í†µê³¼ {passed}ê±´, ì‹¤íŒ¨ {failed}ê±´")
    
    def log_risk_distribution(self, risk_counts: Dict[RiskLevel, int]):
        """ìœ„í—˜ë„ ë¶„í¬ ë¡œê·¸"""
        self.logger.info("ìœ„í—˜ë„ ë¶„í¬:")
        for risk, count in risk_counts.items():
            self.logger.info(f"  - {risk.value}: {count}ê±´")
    
    def log_group(self, group: CooperativeGroup):
        """ê·¸ë£¹ ë¡œê·¸"""
        self.logger.warning(f"ê³µëª¨ ê·¸ë£¹ íƒì§€: {group.group_id}")
        self.logger.warning(f"  - ë©¤ë²„: {', '.join(group.members)}")
        self.logger.warning(f"  - ìœ„í—˜ë„: {group.risk_level.value}")
        self.logger.warning(f"  - ì´ PnL: ${group.pnl_total:.2f}")


# ============================================================================
# 3. DATA PIPELINE
# ============================================================================
# DataLoader removed - using common.data_manager.DataManager singleton instead.


# ============================================================================
# 4. CANDIDATE EXTRACTOR
# ============================================================================

class CandidateExtractor:
    """í›„ë³´ ê±°ë˜ ìŒ ì¶”ì¶œ"""
    
    def __init__(self, con: dd.DuckDBPyConnection, config: DetectionConfig):
        self.con = con
        self.config = config
    
    def extract_candidates(self) -> List[Dict]:
        """SQLì„ í†µí•´ í›„ë³´ ê±°ë˜ ìŒ ì¶”ì¶œ"""
        print("í›„ë³´ ê±°ë˜ ìŒ ì¶”ì¶œ ì¤‘...")
        
        # ì£¼ìš” ì‹¬ë³¼ ì œì™¸ ì¡°ê±´
        exclude_clause = ""
        if self.config.exclude_major_symbols:
            symbols_str = "', '".join(self.config.major_symbols)
            exclude_clause = f"AND t1.symbol NOT IN ('{symbols_str}')"
        
        query = f"""
        WITH position AS (
            SELECT
                account_id,
                position_id,
                MAX(leverage) AS leverage,
                CAST(MIN(ts) AS TIMESTAMP) as open_ts,
                CAST(MAX(ts) AS TIMESTAMP) as closing_ts,
                MAX(symbol) as symbol,
                MAX(side) as side,
                DATE(MAX(ts)) as closing_day,
                SUM(CASE WHEN openclose='OPEN' THEN amount ELSE 0 END) as amount,
                SUM(
                    CASE WHEN openclose='OPEN' THEN -amount ELSE amount END * 
                    CASE WHEN side='LONG' THEN 1 ELSE -1 END
                ) as rpnl
            FROM Trade
            GROUP BY account_id, position_id
        ),
        joined AS (
            SELECT
                t1.account_id AS account_id1,
                t2.account_id AS account_id2,
                t1.symbol,
                t1.open_ts AS open_ts1,
                t2.open_ts AS open_ts2,
                t1.closing_ts AS closing_ts1,
                t2.closing_ts AS closing_ts2,
                t1.leverage,
                t1.amount AS amount1,
                t2.amount AS amount2,
                t1.position_id AS position_id1,
                t2.position_id AS position_id2,
                t1.side as side1,
                t2.side as side2,
                t1.rpnl as rpnl1,
                t2.rpnl as rpnl2,
                ABS(epoch(t1.open_ts) - epoch(t2.open_ts)) AS open_time_diff_sec,
                ABS(epoch(t1.closing_ts) - epoch(t2.closing_ts)) AS close_time_diff_sec
            FROM position t1 
            INNER JOIN position t2 ON
                t1.symbol = t2.symbol
                AND ABS(epoch(t1.open_ts) - epoch(t2.open_ts)) <= {self.config.max_open_time_diff_min * 60}
                AND ABS(epoch(t1.closing_ts) - epoch(t2.closing_ts)) <= {self.config.max_close_time_diff_min * 60}
                AND t1.open_ts < t2.open_ts
                AND GREATEST(t1.open_ts, t2.open_ts) < LEAST(t1.closing_ts, t2.closing_ts)
                AND t1.account_id != t2.account_id
                AND t1.side = t2.side
                {exclude_clause}
        )
        SELECT DISTINCT *
        FROM joined
        ORDER BY symbol, open_ts1
        """
        
        df = self.con.execute(query).fetchdf()
        print(f"í›„ë³´ ê±°ë˜ ìŒ {len(df)}ê°œ ì¶”ì¶œ")
        
        return df.to_dict('records')


# ============================================================================
# 5. FILTER ENGINE
# ============================================================================

class FilterEngine:
    """í•„ìˆ˜ ì¡°ê±´ í•„í„°ë§"""
    
    def __init__(self, config: DetectionConfig):
        self.config = config
    
    def apply_filters(self, candidates: List[Dict]) -> Tuple[List[TradePair], List[Dict]]:
        """í•„í„° ì ìš© ë° TradePair ê°ì²´ ìƒì„±"""
        print("í•„í„° ì—”ì§„ ì‹œì‘...")
        
        passed_pairs = []
        failed_data = []
        
        for idx, row in enumerate(candidates):
            pair_id = f"COOP_{idx:06d}"
            failures = []
            
            # ê¸°ë³¸ì ìœ¼ë¡œ SQLì—ì„œ ì´ë¯¸ í•„í„°ë§ë¨
            # ì¶”ê°€ í•„í„°ê°€ í•„ìš”í•œ ê²½ìš° ì—¬ê¸°ì— ì¶”ê°€
            
            # TradePair ê°ì²´ ìƒì„±
            trade_pair = self._create_trade_pair(pair_id, row)
            trade_pair.passed_filter = True
            passed_pairs.append(trade_pair)
        
        print(f"í•„í„° ì™„ë£Œ: {len(passed_pairs)}/{len(candidates)} í†µê³¼")
        
        return passed_pairs, failed_data
    
    def _create_trade_pair(self, pair_id: str, row: Dict) -> TradePair:
        """TradePair ê°ì²´ ìƒì„±"""
        rpnl1 = row['rpnl1']
        rpnl2 = row['rpnl2']
        
        # ìŠ¹ì/íŒ¨ì ê²°ì •
        if rpnl1 > rpnl2:
            pnl_winner = row['account_id1']
            pnl_loser = row['account_id2']
        else:
            pnl_winner = row['account_id2']
            pnl_loser = row['account_id1']
        
        return TradePair(
            pair_id=pair_id,
            account_id1=row['account_id1'],
            account_id2=row['account_id2'],
            symbol=row['symbol'],
            side1=row['side1'],
            side2=row['side2'],
            open_ts1=row['open_ts1'],
            open_ts2=row['open_ts2'],
            closing_ts1=row['closing_ts1'],
            closing_ts2=row['closing_ts2'],
            open_time_diff_sec=row['open_time_diff_sec'],
            close_time_diff_sec=row['close_time_diff_sec'],
            amount1=row['amount1'],
            amount2=row['amount2'],
            leverage=row['leverage'],
            position_id1=row['position_id1'],
            position_id2=row['position_id2'],
            rpnl1=rpnl1,
            rpnl2=rpnl2,
            total_pnl=rpnl1 + rpnl2,
            pnl_winner=pnl_winner,
            pnl_loser=pnl_loser,
        )


# ============================================================================
# 6. SCORING ENGINE
# ============================================================================

class ScoringEngine:
    """ì ìˆ˜ ê³„ì‚° ë° ìœ„í—˜ë„ ë¶„ë¥˜"""
    
    def __init__(self, config: DetectionConfig, ip_data: pd.DataFrame):
        self.config = config
        self.ip_data = ip_data
        
        # IP ë§¤í•‘ ìƒì„± (account_id -> set of IPs)
        self.account_ips = self._build_ip_mapping()
    
    def _build_ip_mapping(self) -> Dict[str, Set[str]]:
        """ê³„ì •ë³„ IP ë§¤í•‘ ìƒì„±"""
        mapping = defaultdict(set)
        for _, row in self.ip_data.iterrows():
            mapping[row['account_id']].add(row['ip'])
        return mapping
    
    def score_all_pairs(self, pairs: List[TradePair]) -> List[TradePair]:
        """ëª¨ë“  ê±°ë˜ ìŒ ì ìˆ˜ ê³„ì‚°"""
        print("ì ìˆ˜ ì—”ì§„ ì‹œì‘...")
        
        for pair in pairs:
            # ê° ì§€í‘œë³„ ì ìˆ˜ ê³„ì‚°
            pnl_score = self._score_pnl_asymmetry(pair)
            time_score = self._score_time_proximity(pair)
            ip_score = self._score_ip_sharing(pair)
            overlap_score = self._score_position_overlap(pair)
            
            # ì ìˆ˜ ì €ì¥
            pair.score = ScoreBreakdown(
                pnl_asymmetry=pnl_score,
                time_proximity=time_score,
                ip_sharing=ip_score,
                position_overlap=overlap_score,
                total=pnl_score + time_score + ip_score + overlap_score
            )
            
            # ìœ„í—˜ë„ ë¶„ë¥˜
            pair.risk_level = self._classify_risk(pair.score.total)
            
            # if self.config.enable_detailed_logging:
            #     print(
            #         f"{pair.pair_id}: ì ìˆ˜={pair.score.total:.1f} "
            #         f"(PnL:{pnl_score:.1f}, Time:{time_score:.1f}, "
            #         f"IP:{ip_score:.1f}, Overlap:{overlap_score:.1f}) "
            #         f"â†’ {pair.risk_level.value}"
            #     )
        
        print(f"ì ìˆ˜ ê³„ì‚° ì™„ë£Œ: {len(pairs)}ê°œ")
        
        return pairs
    
    def _score_pnl_asymmetry(self, pair: TradePair) -> float:
        """PnL ë¹„ëŒ€ì¹­ì„± ì ìˆ˜ (35ì ) - í•œìª½ì´ í° ì´ìµ"""
        max_weight = self.config.weight_pnl_asymmetry
        
        total_pnl = pair.rpnl1 + pair.rpnl2
        max_pnl = max(abs(pair.rpnl1), abs(pair.rpnl2))
        
        if max_pnl == 0:
            return 0.0
        
        # ë¹„ëŒ€ì¹­ ë¹„ìœ¨: í´ìˆ˜ë¡ í•œìª½ë§Œ ì´ë“
        asymmetry_ratio = abs(total_pnl) / max_pnl
        
        if asymmetry_ratio >= 0.8:  # 80% ì´ìƒ: í•œìª½ë§Œ í° ì´ë“
            return max_weight
        elif asymmetry_ratio >= 0.6:
            return max_weight * 0.75
        elif asymmetry_ratio >= 0.4:
            return max_weight * 0.50
        elif asymmetry_ratio >= 0.2:
            return max_weight * 0.25
        else:
            return 0.0
    
    def _score_time_proximity(self, pair: TradePair) -> float:
        """ì‹œê°„ ê·¼ì ‘ë„ ì ìˆ˜ (25ì )"""
        max_weight = self.config.weight_time_proximity
        
        avg_diff = (pair.open_time_diff_sec + pair.close_time_diff_sec) / 2
        
        if avg_diff <= 5:  # 5ì´ˆ ì´ë‚´
            return max_weight
        elif avg_diff <= 15:  # 15ì´ˆ ì´ë‚´
            return max_weight * 0.80
        elif avg_diff <= 30:  # 30ì´ˆ ì´ë‚´
            return max_weight * 0.60
        elif avg_diff <= 60:  # 1ë¶„ ì´ë‚´
            return max_weight * 0.40
        else:  # 2ë¶„ ì´ë‚´
            return max_weight * 0.20
    
    def _score_ip_sharing(self, pair: TradePair) -> float:
        """IP ê³µìœ  ì ìˆ˜ (25ì )"""
        max_weight = self.config.weight_ip_sharing
        
        ips1 = self.account_ips.get(pair.account_id1, set())
        ips2 = self.account_ips.get(pair.account_id2, set())
        
        shared_ips = ips1 & ips2
        shared_count = len(shared_ips)
        
        if shared_count >= 5:
            return max_weight
        elif shared_count >= 3:
            return max_weight * 0.80
        elif shared_count >= 2:
            return max_weight * 0.60
        elif shared_count >= 1:
            return max_weight * 0.40
        else:
            return 0.0
    
    def _score_position_overlap(self, pair: TradePair) -> float:
        """í¬ì§€ì…˜ ê²¹ì¹¨ ì ìˆ˜ (15ì )"""
        max_weight = self.config.weight_position_overlap
        
        # ì˜¤í”ˆ/í´ë¡œì¦ˆ ì‹œê°„ ê²¹ì¹¨ ê³„ì‚°
        overlap_start = max(pair.open_ts1, pair.open_ts2)
        overlap_end = min(pair.closing_ts1, pair.closing_ts2)
        
        if overlap_end <= overlap_start:
            return 0.0
        
        overlap_duration = (overlap_end - overlap_start).total_seconds()
        total_duration1 = (pair.closing_ts1 - pair.open_ts1).total_seconds()
        total_duration2 = (pair.closing_ts2 - pair.open_ts2).total_seconds()
        min_duration = min(total_duration1, total_duration2)
        
        if min_duration == 0:
            return 0.0
        
        overlap_ratio = overlap_duration / min_duration
        
        if overlap_ratio >= 0.9:
            return max_weight
        elif overlap_ratio >= 0.7:
            return max_weight * 0.75
        elif overlap_ratio >= 0.5:
            return max_weight * 0.50
        else:
            return max_weight * 0.25
    
    def _classify_risk(self, total_score: float) -> RiskLevel:
        """ì ìˆ˜ ê¸°ë°˜ ìœ„í—˜ë„ ë¶„ë¥˜"""
        if total_score >= self.config.critical_threshold:
            return RiskLevel.CRITICAL
        elif total_score >= self.config.high_threshold:
            return RiskLevel.HIGH
        elif total_score >= self.config.medium_threshold:
            return RiskLevel.MEDIUM
        else:
            return RiskLevel.LOW


# ============================================================================
# 7. NETWORK ANALYZER
# ============================================================================

class NetworkAnalyzer:
    """ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ë° ê·¸ë£¹ íƒì§€"""
    
    def __init__(self, config: DetectionConfig, ip_data: pd.DataFrame):
        self.config = config
        self.ip_data = ip_data
    
    def find_groups(self, pairs: List[TradePair]) -> List[CooperativeGroup]:
        """ì—°ê²°ëœ ê³„ì • ê·¸ë£¹ ì°¾ê¸°"""
        print("ë„¤íŠ¸ì›Œí¬ ê·¸ë£¹ íƒìƒ‰ ì¤‘...")
        
        # ê³„ì • ìŒ ìˆ˜ì§‘
        unique_pairs = set()
        for pair in pairs:
            sorted_accounts = tuple(sorted([pair.account_id1, pair.account_id2]))
            unique_pairs.add(sorted_accounts)
        
        # Union-Find ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ê·¸ë£¹ ì°¾ê¸°
        groups = []
        
        for a, b in unique_pairs:
            found = []
            for g in groups:
                if a in g or b in g:
                    g.update([a, b])
                    found.append(g)
            
            if not found:
                groups.append(set([a, b]))
            elif len(found) > 1:
                merged = set().union(*found)
                groups = [g for g in groups if g not in found]
                groups.append(merged)
        
        # CooperativeGroup ê°ì²´ ìƒì„±
        cooperative_groups = []
        
        for idx, group_set in enumerate(groups):
            if len(group_set) < self.config.min_group_size:
                continue
            
            group_members = sorted(list(group_set))
            
            # ê·¸ë£¹ ê´€ë ¨ ê±°ë˜ ìŒ ì°¾ê¸°
            group_pairs = [
                p for p in pairs
                if p.account_id1 in group_set or p.account_id2 in group_set
            ]
            
            # PnL ê³„ì‚° (position_id ì¤‘ë³µ ì œê±°)
            # AD_2.py ë¡œì§ ì°¸ê³ : ë™ì¼ position_idê°€ ì—¬ëŸ¬ pairì— ë‚˜íƒ€ë‚  ìˆ˜ ìˆì–´ ì¤‘ë³µ ì œê±° í•„ìš”
            unique_rpnl = []
            
            # position_id1ë“¤ì˜ rpnl (ì¤‘ë³µ ì œê±°)
            seen_position_ids = set()
            for p in group_pairs:
                if p.position_id1 not in seen_position_ids:
                    unique_rpnl.append(p.rpnl1)
                    seen_position_ids.add(p.position_id1)
            
            # position_id2ë“¤ì˜ rpnl (ì¤‘ë³µ ì œê±°)
            for p in group_pairs:
                if p.position_id2 not in seen_position_ids:
                    unique_rpnl.append(p.rpnl2)
                    seen_position_ids.add(p.position_id2)
            
            # í•©ê³„ ê³„ì‚°
            total_pos = sum(max(0, rpnl) for rpnl in unique_rpnl)
            total_neg = sum(min(0, rpnl) for rpnl in unique_rpnl)
            total_pnl = total_pos + total_neg
            
            # IP ê³µìœ  ë¶„ì„
            shared_ip_info = self._analyze_shared_ips(group_members)
            
            # í‰ê·  ì ìˆ˜ ê³„ì‚°
            scores = [p.score.total for p in group_pairs]
            avg_score = sum(scores) / len(scores) if scores else 0.0
            max_score = max(scores) if scores else 0.0
            
            group = CooperativeGroup(
                group_id=f"GROUP_{idx:04d}",
                members=group_members,
                trade_pair_ids=[p.pair_id for p in group_pairs],
                trade_count=len(group_pairs),
                pnl_positive_sum=total_pos,
                pnl_negative_sum=total_neg,
                pnl_total=total_pnl,
                shared_ip_count=len(shared_ip_info),
                shared_ips=shared_ip_info,
                avg_score=avg_score,
                max_score=max_score,
                risk_level=self._classify_group_risk(avg_score, len(shared_ip_info))
            )
            
            cooperative_groups.append(group)
        
        # PnL ê¸°ì¤€ ì •ë ¬
        cooperative_groups.sort(key=lambda x: x.pnl_total, reverse=True)
        
        print(f"ê·¸ë£¹ íƒìƒ‰ ì™„ë£Œ: {len(cooperative_groups)}ê°œ ê·¸ë£¹")
        
        return cooperative_groups
    
    def _analyze_shared_ips(self, members: List[str]) -> Dict[str, List[str]]:
        """ê·¸ë£¹ ë‚´ ê³µìœ  IP ë¶„ì„"""
        group_ips = self.ip_data[self.ip_data['account_id'].isin(members)]
        ip_counter = Counter(group_ips['ip'])
        
        shared_ips = {}
        for ip, count in ip_counter.items():
            if count > 1:
                accounts = group_ips[group_ips['ip'] == ip]['account_id'].tolist()
                shared_ips[ip] = accounts
        
        return shared_ips
    
    def _classify_group_risk(self, avg_score: float, shared_ip_count: int) -> RiskLevel:
        """ê·¸ë£¹ ìœ„í—˜ë„ ë¶„ë¥˜"""
        # IP ê³µìœ ê°€ ë§ìœ¼ë©´ ìœ„í—˜ë„ ìƒìŠ¹
        bonus = shared_ip_count * 5
        adjusted_score = avg_score + bonus
        
        if adjusted_score >= 85:
            return RiskLevel.CRITICAL
        elif adjusted_score >= 70:
            return RiskLevel.HIGH
        elif adjusted_score >= 50:
            return RiskLevel.MEDIUM
        else:
            return RiskLevel.LOW


# ============================================================================
# 8. SANCTION PIPELINE
# ============================================================================

class SanctionPipeline:
    """ì œì¬ ì¼€ì´ìŠ¤ ìƒì„± ë° ì¶œë ¥"""
    
    def __init__(self, config: DetectionConfig, logger: DetectionLogger):
        self.config = config
        self.logger = logger
        self.sanction_cases: List[SanctionCase] = []
    
    def process_critical_groups(self, groups: List[CooperativeGroup]) -> List[SanctionCase]:
        """Critical ê·¸ë£¹ ì¦‰ì‹œ ì œì¬ ì¼€ì´ìŠ¤ ìƒì„±"""
        print("Critical ê·¸ë£¹ ì œì¬ ìƒì„± ì¤‘...")
        
        critical_groups = [g for g in groups if g.risk_level == RiskLevel.CRITICAL]
        
        if len(critical_groups) == 0:
            print("Critical ê·¸ë£¹ ì—†ìŒ")
            return []
        
        for group in critical_groups:
            sanction = SanctionCase(
                case_id=f"SANCTION_CRITICAL_{group.group_id}",
                sanction_type=SanctionType.IMMEDIATE_CRITICAL,
                group_id=group.group_id,
                account_ids=group.members,
                detection_timestamp=datetime.now(),
                trade_pair_ids=group.trade_pair_ids,
                total_score=group.max_score,
                risk_level=RiskLevel.CRITICAL,
                shared_ip_count=group.shared_ip_count,
                shared_ips=group.shared_ips,
                total_pnl=group.pnl_total,
                pnl_positive_sum=group.pnl_positive_sum,
                pnl_negative_sum=group.pnl_negative_sum,
                evidence_summary=f"í™•ì‹¤í•œ ê³µëª¨ ê±°ë˜ íŒ¨í„´ (ì ìˆ˜: {group.max_score:.1f}/100, ê±°ë˜: {group.trade_count}ê±´)"
            )
            
            self.sanction_cases.append(sanction)
            self.logger.logger.warning(f"ì œì¬ ì¼€ì´ìŠ¤ ìƒì„±: {sanction.case_id}")
        
        print(f"Critical ì œì¬: {len(critical_groups)}ê±´")
        
        return [c for c in self.sanction_cases if c.sanction_type == SanctionType.IMMEDIATE_CRITICAL]
    
    def process_ip_shared_groups(self, groups: List[CooperativeGroup]) -> List[SanctionCase]:
        """IP ê³µìœ  ë„¤íŠ¸ì›Œí¬ ì œì¬ ì¼€ì´ìŠ¤ ìƒì„±"""
        print("IP ê³µìœ  ë„¤íŠ¸ì›Œí¬ ì œì¬ ì¼€ì´ìŠ¤ ìƒì„± ì¤‘...")
        
        # HIGH ìœ„í—˜ë„ + IP ê³µìœ ê°€ ìˆëŠ” ê·¸ë£¹
        ip_shared_groups = [
            g for g in groups
            if g.risk_level == RiskLevel.HIGH 
            and g.shared_ip_count >= self.config.min_shared_ips
        ]
        
        ip_sanctions = []
        
        for group in ip_shared_groups:
            # ì´ë¯¸ Criticalë¡œ ì œì¬ëœ ê²½ìš° ìŠ¤í‚µ
            if any(c.group_id == group.group_id and c.sanction_type == SanctionType.IMMEDIATE_CRITICAL 
                   for c in self.sanction_cases):
                continue
            
            sanction = SanctionCase(
                case_id=f"SANCTION_IP_{group.group_id}",
                sanction_type=SanctionType.IP_SHARED_NETWORK,
                group_id=group.group_id,
                account_ids=group.members,
                detection_timestamp=datetime.now(),
                trade_pair_ids=group.trade_pair_ids,
                total_score=group.avg_score,
                risk_level=RiskLevel.HIGH,
                shared_ip_count=group.shared_ip_count,
                shared_ips=group.shared_ips,
                total_pnl=group.pnl_total,
                pnl_positive_sum=group.pnl_positive_sum,
                pnl_negative_sum=group.pnl_negative_sum,
                evidence_summary=f"IP ê³µìœ  ë„¤íŠ¸ì›Œí¬ ({group.shared_ip_count}ê°œ IP ê³µìœ , ê±°ë˜: {group.trade_count}ê±´)"
            )
            
            ip_sanctions.append(sanction)
            self.sanction_cases.append(sanction)
            self.logger.logger.warning(f"ì œì¬ ì¼€ì´ìŠ¤ ìƒì„±: {sanction.case_id}")
        
        print(f"IP ê³µìœ  ì œì¬: {len(ip_sanctions)}ê±´")
        
        return ip_sanctions
    
    def export_sanctions(self, output_dir: Path) -> str:
        """ì œì¬ ì¼€ì´ìŠ¤ë¥¼ JSON íŒŒì¼ë¡œ ì¶œë ¥"""
        output_dir.mkdir(parents=True, exist_ok=True)
        filepath = output_dir / "sanction_groups.json"
        
        data = {
            'total_sanction_groups': len(self.sanction_cases),
            'generated_at': datetime.now().isoformat(),
            'sanctions': [case.to_dict() for case in self.sanction_cases]
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        
        print(f"ì œì¬ ì¼€ì´ìŠ¤ ì €ì¥: {filepath} ({len(self.sanction_cases)}ê±´)")
        
        return str(filepath)


# ============================================================================
# 9. REPORTING & VISUALIZATION
# ============================================================================

class ReportGenerator:
    """ë¶„ì„ ë³´ê³ ì„œ ë° ì‹œê°í™” ë°ì´í„° ìƒì„±"""
    
    def __init__(self, config: DetectionConfig):
        self.config = config
        self.output_dir = Path(config.output_dir)
    
    def generate_all_reports(
        self,
        all_pairs: List[TradePair],
        groups: List[CooperativeGroup]
    ):
        """ëª¨ë“  ë³´ê³ ì„œ ìƒì„±"""
        print("ë³´ê³ ì„œ ìƒì„± ì¤‘...")
        
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # 1. ê±°ë˜ ìŒ ìƒì„¸ ë°ì´í„° (CSV)
        self._export_trade_pairs_csv(all_pairs)
        
        # 2. ê·¸ë£¹ ìƒì„¸ ë°ì´í„° (CSV)
        self._export_groups_csv(groups)
        
        # 3. ì‹œê°í™”ìš© JSON ë°ì´í„°
        self._export_visualization_data(all_pairs, groups)
        
        # 4. ìš”ì•½ ë³´ê³ ì„œ (í…ìŠ¤íŠ¸)
        self._generate_summary_report(all_pairs, groups)
        
        print("ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ")
    
    def _export_trade_pairs_csv(self, pairs: List[TradePair]):
        """ê±°ë˜ ìŒ ìƒì„¸ CSV"""
        if not pairs:
            return
        
        records = []
        for pair in pairs:
            record = {
                'pair_id': pair.pair_id,
                'account_id1': pair.account_id1,
                'account_id2': pair.account_id2,
                'risk_level': pair.risk_level.value,
                'total_score': pair.score.total,
                'symbol': pair.symbol,
                'rpnl1': pair.rpnl1,
                'rpnl2': pair.rpnl2,
                'total_pnl': pair.total_pnl,
                'pnl_winner': pair.pnl_winner,
                'pnl_loser': pair.pnl_loser,
                'open_time_diff_sec': pair.open_time_diff_sec,
                'close_time_diff_sec': pair.close_time_diff_sec,
                'score_pnl_asymmetry': pair.score.pnl_asymmetry,
                'score_time_proximity': pair.score.time_proximity,
                'score_ip_sharing': pair.score.ip_sharing,
                'score_position_overlap': pair.score.position_overlap,
            }
            records.append(record)
        
        df = pd.DataFrame(records)
        filepath = self.output_dir / "trade_pairs_detailed.csv"
        df.to_csv(filepath, index=False, encoding='utf-8-sig')
        print(f"ê±°ë˜ ìŒ CSV ì €ì¥: {filepath}")
    
    def _export_groups_csv(self, groups: List[CooperativeGroup]):
        """ê·¸ë£¹ ìƒì„¸ CSV"""
        if not groups:
            return
        
        records = []
        for group in groups:
            record = {
                'group_id': group.group_id,
                'members': ', '.join(group.members),
                'member_count': len(group.members),
                'trade_count': group.trade_count,
                'risk_level': group.risk_level.value,
                'avg_score': group.avg_score,
                'max_score': group.max_score,
                'pnl_positive_sum': group.pnl_positive_sum,
                'pnl_negative_sum': group.pnl_negative_sum,
                'pnl_total': group.pnl_total,
                'shared_ip_count': group.shared_ip_count,
            }
            records.append(record)
        
        df = pd.DataFrame(records)
        filepath = self.output_dir / "cooperative_groups.csv"
        df.to_csv(filepath, index=False, encoding='utf-8-sig')
        print(f"ê·¸ë£¹ CSV ì €ì¥: {filepath}")
    
    def _export_visualization_data(
        self, 
        pairs: List[TradePair],
        groups: List[CooperativeGroup]
    ):
        """ì‹œê°í™”ìš© JSON ë°ì´í„°"""
        
        # ìœ„í—˜ë„ë³„ ë¶„í¬
        risk_counts = defaultdict(int)
        for pair in pairs:
            risk_counts[pair.risk_level.value] += 1
        
        # ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ë°ì´í„°
        nodes = set()
        edges = []
        
        for pair in pairs:
            if pair.risk_level in [RiskLevel.CRITICAL, RiskLevel.HIGH]:
                nodes.add(pair.account_id1)
                nodes.add(pair.account_id2)
                
                edges.append({
                    'source': pair.account_id1,
                    'target': pair.account_id2,
                    'value': abs(pair.total_pnl),
                    'risk_level': pair.risk_level.value,
                    'score': pair.score.total,
                })
        
        vis_data = {
            'summary': {
                'total_pairs': len(pairs),
                'critical': risk_counts.get('CRITICAL', 0),
                'high': risk_counts.get('HIGH', 0),
                'medium': risk_counts.get('MEDIUM', 0),
                'low': risk_counts.get('LOW', 0),
                'total_groups': len(groups),
                'total_pnl': sum(g.pnl_total for g in groups),
            },
            'risk_distribution': dict(risk_counts),
            'network_graph': {
                'nodes': [{'id': node} for node in nodes],
                'edges': edges,
            },
            'group_stats': [
                {
                    'group_id': g.group_id,
                    'member_count': len(g.members),
                    'pnl_total': g.pnl_total,
                    'risk_level': g.risk_level.value,
                }
                for g in groups
            ]
        }
        
        filepath = self.output_dir / "visualization_data.json"
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(vis_data, f, indent=2, ensure_ascii=False)
        
        print(f"ì‹œê°í™” ë°ì´í„° ì €ì¥: {filepath}")
    
    def _generate_summary_report(
        self,
        pairs: List[TradePair],
        groups: List[CooperativeGroup]
    ):
        """ìš”ì•½ ë³´ê³ ì„œ í…ìŠ¤íŠ¸"""
        
        risk_counts = defaultdict(int)
        for pair in pairs:
            risk_counts[pair.risk_level] += 1
        
        total_pnl = sum(g.pnl_total for g in groups)
        groups_with_shared_ip = sum(1 for g in groups if g.shared_ip_count > 0)
        
        # ìƒìœ„ ê·¸ë£¹
        top_groups = groups[:5]
        
        report = f"""
{'='*70}
ê³µëª¨ê±°ë˜ íƒì§€ ë³´ê³ ì„œ
{'='*70}

ğŸ“Š íƒì§€ ìš”ì•½
  - ì´ ì˜ì‹¬ ê±°ë˜ ìŒ: {len(pairs)}ê±´
  - Critical (í™•ì‹¤í•œ ê³µëª¨): {risk_counts[RiskLevel.CRITICAL]}ê±´
  - High (ë†’ì€ ì˜ì‹¬): {risk_counts[RiskLevel.HIGH]}ê±´
  - Medium (ì¤‘ê°„ ì˜ì‹¬): {risk_counts[RiskLevel.MEDIUM]}ê±´
  - Low (ë‚®ì€ ì˜ì‹¬): {risk_counts[RiskLevel.LOW]}ê±´

ğŸ‘¥ ê·¸ë£¹ ë¶„ì„
  - íƒì§€ëœ ê·¸ë£¹: {len(groups)}ê°œ
  - IP ê³µìœ  ê·¸ë£¹: {groups_with_shared_ip}ê°œ
  - ì´ ìˆœìˆ˜ìµ: ${total_pnl:,.2f}

ğŸ¯ ìƒìœ„ ê·¸ë£¹ (Top 5)
"""
        
        for idx, group in enumerate(top_groups, 1):
            report += f"""  {idx}. {group.group_id}
     - ë©¤ë²„: {', '.join(group.members[:5])}{'...' if len(group.members) > 5 else ''}
     - ë©¤ë²„ ìˆ˜: {len(group.members)}ëª…
     - ê±°ë˜ ìˆ˜: {group.trade_count}ê±´
     - ìˆœìˆ˜ìµ: ${group.pnl_total:,.2f}
     - ìœ„í—˜ë„: {group.risk_level.value}
     - ê³µìœ  IP: {group.shared_ip_count}ê°œ
"""
        
        report += f"""
{'='*70}
"""
        
        filepath = self.output_dir / "summary_report.txt"
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"ìš”ì•½ ë³´ê³ ì„œ ì €ì¥: {filepath}")
        print(report)


# ============================================================================
# 10. MAIN DETECTOR ENGINE
# ============================================================================

class CooperativeTradingDetector:
    """ë©”ì¸ íƒì§€ ì—”ì§„"""
    
    def __init__(self, config: DetectionConfig):
        self.config = config
        self.logger = DetectionLogger(config)
        self.sanction_pipeline = SanctionPipeline(config, self.logger)
    
    def detect(self, data_filepath: str) -> Dict:
        """ì „ì²´ íƒì§€ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        
        # 1. ë°ì´í„° ë¡œë“œ (ê³µí†µ DataManager ì‚¬ìš©)
        self.logger.log_phase("ë°ì´í„° ë¡œë“œ")
        # detectors should use the persistent DuckDB file created by main
        db_path = Path.cwd() / 'data' / 'ingest.duckdb'
        con = dd.connect(database=str(db_path))

        # load auxiliary tables used by non-SQL parts of the detector
        data = {}
        try:
            data['IP'] = con.execute('SELECT * FROM "IP"').fetchdf()
        except Exception:
            data['IP'] = pd.DataFrame()
        
        # 2. í›„ë³´ ì¶”ì¶œ
        self.logger.log_phase("í›„ë³´ ê±°ë˜ ìŒ ì¶”ì¶œ")
        extractor = CandidateExtractor(con, self.config)
        candidates = extractor.extract_candidates()
        
        if len(candidates) == 0:
            print("í›„ë³´ ê±°ë˜ ìŒì´ ì—†ìŠµë‹ˆë‹¤. íƒì§€ ì¢…ë£Œ.")
            return self._empty_result()
        
        # 3. í•„í„° ì ìš©
        self.logger.log_phase("í•„í„° ì ìš©")
        filter_engine = FilterEngine(self.config)
        passed_pairs, failed_pairs = filter_engine.apply_filters(candidates)
        self.logger.log_filter_result(len(candidates), len(passed_pairs), len(failed_pairs))
        
        if len(passed_pairs) == 0:
            print("í•„í„°ë¥¼ í†µê³¼í•œ ê±°ë˜ ìŒì´ ì—†ìŠµë‹ˆë‹¤.")
            return self._empty_result()
        
        # 4. ì ìˆ˜ ê³„ì‚°
        self.logger.log_phase("ì ìˆ˜ ê³„ì‚° ë° ìœ„í—˜ë„ ë¶„ë¥˜")
        scoring_engine = ScoringEngine(self.config, data['IP'])
        scored_pairs = scoring_engine.score_all_pairs(passed_pairs)
        
        # ìœ„í—˜ë„ ë¶„í¬ ë¡œê¹…
        risk_counts = defaultdict(int)
        for pair in scored_pairs:
            risk_counts[pair.risk_level] += 1
        self.logger.log_risk_distribution(risk_counts)
        
        # 5. ë„¤íŠ¸ì›Œí¬ ë¶„ì„
        self.logger.log_phase("ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ë° ê·¸ë£¹ íƒì§€")
        network_analyzer = NetworkAnalyzer(self.config, data['IP'])
        groups = network_analyzer.find_groups(scored_pairs)
        
        # 6. Critical ê·¸ë£¹ ì¦‰ì‹œ ì œì¬
        self.logger.log_phase("Critical ê·¸ë£¹ ì œì¬")
        critical_sanctions = self.sanction_pipeline.process_critical_groups(groups)
        
        # 7. IP ê³µìœ  ë„¤íŠ¸ì›Œí¬ ì œì¬
        self.logger.log_phase("IP ê³µìœ  ë„¤íŠ¸ì›Œí¬ ì œì¬")
        ip_sanctions = self.sanction_pipeline.process_ip_shared_groups(groups)
        
        # 8. ì œì¬ ì¼€ì´ìŠ¤ ì¶œë ¥
        self.logger.log_phase("ì œì¬ ì¼€ì´ìŠ¤ ì¶œë ¥")
        sanction_file = self.sanction_pipeline.export_sanctions(Path(self.config.output_dir))
        
        # 9. ë³´ê³ ì„œ ìƒì„±
        self.logger.log_phase("ë³´ê³ ì„œ ìƒì„±")
        report_generator = ReportGenerator(self.config)
        report_generator.generate_all_reports(scored_pairs, groups)
        
        # 10. ê²°ê³¼ ë°˜í™˜
        all_sanctions = critical_sanctions + ip_sanctions
        return {
            'config': self.config.to_dict(),
            'total_candidates': len(candidates),
            'passed_filter': len(passed_pairs),
            'risk_distribution': {k.value: v for k, v in risk_counts.items()},
            'total_groups': len(groups),
            'total_pnl': sum(g.pnl_total for g in groups),
            'sanction_cases': len(all_sanctions),
            'critical_sanctions': len(critical_sanctions),
            'ip_sanctions': len(ip_sanctions),
            'output_directory': self.config.output_dir,
        }
    
    def _empty_result(self) -> Dict:
        """ë¹ˆ ê²°ê³¼ ë°˜í™˜"""
        return {
            'config': self.config.to_dict(),
            'total_candidates': 0,
            'passed_filter': 0,
            'risk_distribution': {},
            'total_groups': 0,
            'total_pnl': 0.0,
            'output_directory': self.config.output_dir,
        }


# ============================================================================
# MAIN EXECUTION
# ============================================================================

def run_detection(
    data_filepath: str,
    config: Optional[DetectionConfig] = None
) -> Dict:
    """
    ê³µëª¨ê±°ë˜ íƒì§€ ì‹¤í–‰
    
    Args:
        data_filepath: Excel ë°ì´í„° íŒŒì¼ ê²½ë¡œ
        config: íƒì§€ ì„¤ì • (Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
    
    Returns:
        íƒì§€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    if config is None:
        config = DetectionConfig()
    
    # ì„¤ì • ì €ì¥
    config.save(str(Path(config.output_dir) / 'detection_config.json'))
    
    # íƒì§€ ì‹¤í–‰
    detector = CooperativeTradingDetector(config)
    result = detector.detect(data_filepath)
    
    return result


if __name__ == "__main__":
    # ì»¤ìŠ¤í…€ ì„¤ì •
    custom_config = DetectionConfig(
        # Filter íŒŒë¼ë¯¸í„°
        max_open_time_diff_min=2.0,
        max_close_time_diff_min=2.0,
        exclude_major_symbols=True,
        
        # ìœ„í—˜ë„ ì„ê³„ê°’
        critical_threshold=85,
        high_threshold=70,
        medium_threshold=50,
        
        # ë„¤íŠ¸ì›Œí¬ íŒŒë¼ë¯¸í„°
        min_group_size=2,
        min_shared_ips=1,
        
        # ì¶œë ¥ ì„¤ì •
        output_dir="./output/cooperative",
        enable_detailed_logging=True
    )
    
    # ì‹¤í–‰
    print("\n" + "="*70)
    print("ê³µëª¨ê±°ë˜ íƒì§€ ì‹œìŠ¤í…œ v2.0")
    print("="*70 + "\n")
    
    result = run_detection(
        data_filepath="problem_data_final.xlsx",
        config=custom_config
    )
    
    # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
    print("\n" + "="*70)
    print("íƒì§€ ì™„ë£Œ!")
    print("="*70)
    print(f"ì´ í›„ë³´: {result['total_candidates']}ê±´")
    print(f"í•„í„° í†µê³¼: {result['passed_filter']}ê±´")
    print(f"\nìœ„í—˜ë„ ë¶„í¬:")
    for risk, count in result['risk_distribution'].items():
        print(f"  - {risk}: {count}ê±´")
    print(f"\nì´ ê·¸ë£¹: {result['total_groups']}ê°œ")
    print(f"ì´ ìˆœìˆ˜ìµ: ${result['total_pnl']:,.2f}")
    print(f"\nê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {result['output_directory']}")
    print("="*70 + "\n")
